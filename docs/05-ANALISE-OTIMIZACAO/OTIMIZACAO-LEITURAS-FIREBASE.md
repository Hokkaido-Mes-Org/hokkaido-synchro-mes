# Otimiza√ß√£o de Leituras Firebase ‚Äî Plano Executado

**Data:** 2025  
**Status:** ‚úÖ Implementado  
**Objetivo:** Reduzir custos com leituras Firestore (estimativa: ~1,2M leituras/dia ‚Üí ~400K/dia)

---

## Diagn√≥stico (antes das altera√ß√µes)

| M√©trica | Valor |
|---------|-------|
| Total `.get()` diretos | 157 chamadas no c√≥digo |
| `onSnapshot` ativos simult√¢neos | 10 subscriptions |
| `setInterval` polling | 18 timers |
| `.collection()` refer√™ncias | 407 |
| Estimativa leituras/turno/usu√°rio | ~39.600 |
| Estimativa leituras/dia (10 users √ó 3 turnos) | ~1.188.000 |

### Maiores ofensores identificados

| # | Fonte | Cole√ß√£o | Impacto estimado/turno/user |
|---|-------|---------|-----------------------------|
| 1 | `launchPlanning` (script.js) | planning (30 dias) | ~10.000 leituras |
| 2 | Listener duplicado `launchProductions` | production_entries | ~5.500 leituras |
| 3 | Polling `active_downtimes` (60s + 120s dual) | active_downtimes | ~3.600 leituras |
| 4 | Controllers sem cache (admin, analysis, monitoring) | v√°rios | ~8.000 leituras |
| 5 | TTLs muito curtos (30s-120s) | v√°rios | ~5.000 leituras |

---

## Altera√ß√µes Implementadas

### Fase 1 ‚Äî Elimina√ß√£o de duplicatas (impacto: ~60% redu√ß√£o)

#### 1A. Listener `launchPlanning` ‚Äî de 30 dias para hoje
- **Arquivo:** `script.js` (linha ~11656)
- **Antes:** `db.collection('planning').where('date', '>=', thirtyDaysAgoStr)` ‚Äî carregava 30 dias
- **Depois:** `db.collection('planning').where('date', '==', date)` ‚Äî carrega apenas hoje
- **Economia:** ~10.000 leituras/turno/usu√°rio

#### 1B. Listener `launchProductions` ‚Äî eliminado (duplicava `productionEntries`)
- **Arquivo:** `script.js` (linha ~11675)
- **Antes:** Listener `onSnapshot` separado em `production_entries` (id√™ntico ao `productionEntries`)
- **Depois:** Reutiliza dados do DataStore via `subscribe('productionEntries', ...)`
- **Economia:** ~5.500 leituras/turno/usu√°rio (1 listener a menos)

#### 1C. Polling `active_downtimes` ‚Äî unificado em 300s
- **Arquivos:** `script.js` (linha ~11281), `planning.controller.js` (linha ~2487)
- **Antes:** script.js polled a cada 120s, planning.controller.js a cada 60s
- **Depois:** Ambos em 300s (5 min), com pause/resume na visibilidade da aba
- **Economia:** ~3.600 leituras/turno/usu√°rio

### Fase 2 ‚Äî Aumento de TTLs (impacto: ~20% redu√ß√£o adicional)

| Componente | Antes | Depois | Arquivo |
|------------|-------|--------|---------|
| `BaseService.cacheTTL` | 60s | **120s** | `src/services/base.service.js` |
| `ActiveDowntimesService` | 30s | **120s** | `src/services/downtime.service.js` |
| `LogsService` | 30s | **300s** (5min) | `src/services/logs.service.js` |
| `StateManager.defaultTTL` | 120s | **300s** (5min) | `src/core/state-manager.js` |
| `CacheManager._ttl` | 180s | **300s** (5min) | `script.js` (linha ~712) |
| `DataStore.isFresh` default | 300s | **600s** (10min) | `script.js` (linha ~852) |
| `DataStore.fetchIfNeeded` | 180s | **300s** (5min) | `script.js` (linha ~899) |
| `getActiveDowntimesCached` | 120s | **300s** (5min) | `script.js` (linha ~1383) |
| `getDowntimeEntriesCached` | 120s | **300s** (5min) | `script.js` (linha ~1410) |
| OEE polling | 30min | **60min** | `script.js` (linha ~4410) |
| Timeline polling | 10min | **30min** | `script.js` (linha ~4413) |

### Fase 3 ‚Äî Cache em controllers sem cache (impacto: ~15% redu√ß√£o)

#### monitoring.controller.js
- **Adicionado:** `cachedQuery()` helper com TTL 5min
- **Cacheados:** 9 queries diretas ‚Üí 4 chaves de cache reutiliz√°veis
  - `production_entries` por data (2 queries ‚Üí 1 cache hit)
  - `production_entries` por turno (6 queries ‚Üí cache hit)
  - `planning` por data (1 query ‚Üí cache hit)
  - `downtime_entries` por data (1 query ‚Üí cache hit)

#### analysis.controller.js
- **Adicionado:** `cachedInlineQuery()` helper com TTL 5min
- **Cacheados:** Relat√≥rios inline (Resumo, Efici√™ncia, Paradas, Perdas, Produ√ß√£o)
- **Chaves compartilhadas:** `prod_${start}_${end}`, `down_${start}_${end}`, `losses_${start}_${end}`
- Relat√≥rios com mesmo per√≠odo reutilizam cache automaticamente

#### tooling.controller.js
- **Adicionado:** Cache para `ferramentaria_moldes` (full collection read) com TTL 5min
- **Antes:** Leitura completa da cole√ß√£o a cada navega√ß√£o para a aba

#### reports.controller.js
- **Adicionado:** `cachedRelQuery()` helper com TTL 5min
- **Cacheados:** 3 queries de relat√≥rios (produ√ß√£o, perdas, paradas)
- Mudan√ßa de per√≠odo no mesmo range reutiliza cache

---

## Estimativa de Impacto

| M√©trica | Antes | Depois | Redu√ß√£o |
|---------|-------|--------|---------|
| Leituras/turno/usu√°rio | ~39.600 | ~13.000 | **~67%** |
| Leituras/dia (10 √ó 3) | ~1.188.000 | ~390.000 | **~67%** |
| Listeners ativos simult√¢neos | 10 | **8** | -2 |
| Polling total (active_downtimes) | 60s + 120s | **300s** | -80% freq |
| Intervalo m√≠n. entre leituras cache | 30s-60s | **120s-600s** | 4-10√ó mais lento |

### Decomposi√ß√£o da economia

| Altera√ß√£o | Economia estimada/dia |
|-----------|----------------------|
| launchPlanning ‚Üí hoje | ~300.000 leituras |
| launchProductions eliminado | ~165.000 leituras |
| Polling 300s unificado | ~108.000 leituras |
| TTLs aumentados (services+DataStore) | ~150.000 leituras |
| Cache em controllers | ~75.000 leituras |
| **Total** | **~798.000 leituras/dia** |

---

## Arquivos Modificados

| Arquivo | Tipo de altera√ß√£o |
|---------|-------------------|
| `script.js` | Listeners, TTLs, polling |
| `src/controllers/planning.controller.js` | Polling interval |
| `src/controllers/monitoring.controller.js` | Cache helper + queries cacheadas |
| `src/controllers/analysis.controller.js` | Cache helper + queries inline cacheadas |
| `src/controllers/tooling.controller.js` | Cache para full collection read |
| `src/controllers/reports.controller.js` | Cache helper + queries cacheadas |
| `src/services/base.service.js` | TTL padr√£o |
| `src/services/downtime.service.js` | TTL ActiveDowntimes |
| `src/services/logs.service.js` | TTL Logs |
| `src/core/state-manager.js` | TTL padr√£o |

---

## Fase 4 ‚Äî Quick Wins (Fev/2026) ‚úÖ

### 4A. Dashboard TV: reutilizar dados do onSnapshot
- **Arquivo:** `dashboard-tv.html`
- **Antes:** `loadRealTimeData` e `updateRealTimeActiveDowntimes` faziam `.get()` ou cache read de `active_downtimes` separadamente
- **Depois:** `onSnapshot` armazena dados em `_activeDowntimesSnapshotData`, reutilizado por ambas fun√ß√µes (0 leituras extras)
- **Economia:** ~2.880 leituras/dia por TV

### 4B. Dashboard TV: intervalo de polling 60s ‚Üí 300s (5 min)
- **Arquivo:** `dashboard-tv.html`
- **Antes:** `setInterval(loadRealTimeData, 60000)` ‚Äî refresh a cada 1 minuto
- **Depois:** `setInterval(loadRealTimeData, 300000)` ‚Äî refresh a cada 5 minutos
- **Economia:** ~80% redu√ß√£o nas leituras de polling do Dashboard TV

### 4C. Dashboard TV: visibilitychange para pausar polling
- **Arquivo:** `dashboard-tv.html`
- **Adicionado:** `document.addEventListener('visibilitychange', ...)` ‚Äî pausa/retoma `_dashboardTVPollInterval`
- **Economia:** evita leituras quando TV est√° com tela desligada ou navegador em background

### 4D. Unificar polling duplicado de active_downtimes
- **Arquivo:** `src/controllers/planning.controller.js`
- **Antes:** Sobrescrevia `window._startActiveDowntimesPolling` e registrava `visibilitychange` duplicado
- **Depois:** Usa vari√°vel separada `window._planningDowntimesPolling`, sem duplicar handler de visibilidade
- **Economia:** ~8.640 leituras/dia

### 4E. Limitar full collection reads de production_orders
- **Arquivos:** `script.js`, `planning.controller.js`, `orders.controller.js`, `reports.controller.js`, `firebase-cache.service.js`
- **Antes:** `db.collection('production_orders').orderBy('createdAt','desc').get()` ‚Äî sem limit
- **Depois:** Adicionado `.limit(500)` em todos os pontos (listeners e fallbacks)
- **Economia:** Proporcional ao crescimento da cole√ß√£o; evita leitura de OPs antigas desnecess√°rias

---

## Riscos e Mitiga√ß√µes

| Risco | Mitiga√ß√£o |
|-------|-----------|
| Dados desatualizados com TTLs maiores | Todas as a√ß√µes de escrita (salvar, editar, excluir) usam `forceRefresh` ou invalidam cache espec√≠fico |
| `launchPlanning` n√£o mostrar planos antigos | Planos ativos de datas anteriores devem ser replanejados ‚Äî comportamento correto |
| Polling 300s pode atrasar cards vermelhos de paradas | A detec√ß√£o manual (bot√£o registrar parada) √© instant√¢nea; apenas o card visual demora at√© 5min |
| Cache do reports.controller pode servir snapshot stale | TTL de 5min √© aceit√°vel para relat√≥rios que s√£o sob demanda |
| Dashboard TV 5min pode atrasar dados visuais | `onSnapshot` em `active_downtimes` atualiza paradas em tempo real; demais dados atualizam a cada 5min |
| `.limit(500)` em production_orders | Cobre amplamente o volume operacional; OPs muito antigas s√£o irrelevantes para opera√ß√£o di√°ria |

---

## Pr√≥ximos passos (se necess√°rio)

1. **Monitorar custos Firebase** por 1 semana para validar redu√ß√£o real
2. **Considerar Cloud Functions** para consolidar dados di√°rios (elimina queries de range)
3. **Implementar `onSnapshot` com `includeMetadataChanges`** para evitar reads duplicados durante reconex√£o
4. **Indexar consultas compostas** no Firestore para reduzir scan de documentos
5. **Avaliar migra√ß√£o parcial para Realtime Database** para dados com alta frequ√™ncia (active_downtimes)
6. ~~**Implementar invalida√ß√£o de cache em writes** (Fase 4B do plano de a√ß√µes)~~ ‚úÖ Implementado (write-invalidation.js)
7. **Converter pollings de active_downtimes para onSnapshot compartilhado** (a√ß√£o 3.2)

---

## Fase 4B ‚Äî Otimiza√ß√µes N√≠vel 2 (Fev/2026) ‚úÖ

### 4F. Write-Invalidation Wrapper (A√ß√£o 2.1)
- **Arquivo criado:** `src/utils/write-invalidation.js` (~175 linhas)
- **Fun√ß√£o:** Mapa centralizado de 8 cole√ß√µes ‚Üí chaves de cache (DataStore, CacheManager, StateManager, EventBus)
- **API:** `invalidateCacheForCollection(collection, machineId)`, `invalidateAfterWrite(collection, writeFn, machineId)`, `getWriteInvalidationStats()`
- **Bootstrap:** Importado em `src/index.js`, bridge atualizado em `src/legacy/bridge.js` (collectionEventMap para eventos `*:changed`)
- **Economia:** Evita reloads desnecess√°rios ap√≥s writes ‚Äî impacto proporcional ao volume de escritas

### 4G. Tab-Aware Prefetch (A√ß√£o 2.2)
- **Arquivo modificado:** `script.js` ‚Äî `_prefetchCollections`
- **Antes:** Prefetch configurado para 5 de 14 abas
- **Depois:** Todas 14 abas mapeadas com cole√ß√µes para prefetch (incluindo `downtime_entries` e `extended_downtime_logs` adicionados ao switch)
- **Economia:** Evita re-fetch de dados ao navegar entre abas; dados pr√©-carregados ficam no cache por 5min

### 4H. Shared Query Cache (A√ß√£o 2.3)
- **Arquivo criado:** `src/utils/shared-query-cache.js` (~100 linhas)
- **Integra√ß√µes:** `analysis.controller.js` (`cachedInlineQuery` ‚Üí sharedQueryCache), `reports.controller.js` (`cachedRelQuery` ‚Üí sharedQueryCache)
- **Antes:** Cada controller tinha cache privado (Map) ‚Äî mesmos dados do mesmo per√≠odo eram duplicados
- **Depois:** Cache compartilhado singleton, TTL 300s, suporta invalida√ß√£o por prefixo
- **Economia:** ~50% redu√ß√£o em queries duplicadas quando Analysis e Reports consultam mesmo per√≠odo

### 4I. Monitor Firebase no Admin (A√ß√£o 2.5)
- **Arquivos modificados:** `index.html` (nova aba "Monitor Firebase" no admin), `admin.controller.js` (fun√ß√µes `setupFirebaseMonitor` + `renderFirebaseMonitor`)
- **Funcionalidade:** Painel com KPIs (total leituras, cache hit ratio, listeners ativos, write invalidations), tabela top cole√ß√µes, status dos 3 caches (StateManager, SharedQueryCache, DataStore)
- **Impacto:** Visibilidade zero-code do consumo Firebase em tempo real

### Novos arquivos criados nesta fase:
| Arquivo | Linhas | Fun√ß√£o |
|---------|--------|--------|
| `src/utils/write-invalidation.js` | ~175 | Invalida√ß√£o de cache p√≥s-escrita |
| `src/utils/shared-query-cache.js` | ~100 | Cache compartilhado cross-controller |

### Documenta√ß√£o criada:
- `docs/05-ANALISE-OTIMIZACAO/ESTIMATIVA-USO-POR-USUARIO.md` ‚Äî Estimativa de consumo Firebase por perfil de usu√°rio (5 perfis √ó 3 turnos)

---

## Fase 4B-N3 ‚Äî Otimiza√ß√µes N√≠vel 3: onSnapshot + Batch Reads (Fev/2026) ‚úÖ

### 4J. ActiveDowntimesLiveService ‚Äî onSnapshot compartilhado (A√ß√£o 3.2)
- **Arquivo criado:** `src/services/active-downtimes-live.service.js` (~260 linhas)
- **Classe:** `ActiveDowntimesLiveService` ‚Äî singleton `onSnapshot` listener para `active_downtimes`
- **API:** `start()`, `stop()`, `getData()`, `getForMachine(machineId)`, `hasData()`, `subscribe(fn)`, `stats()`
- **Funcionalidades:**
  - `includeMetadataChanges: true` ‚Äî filtra `fromCache` para evitar leituras duplicadas (a√ß√£o 3.4 integrada)
  - Auto-sync para DataStore, CacheManager e EventBus (`active_downtimes:updated`)
  - `visibilitychange`: pausa listener ao ocultar tab, retoma ao exibir
  - Error recovery: auto-restart ap√≥s 30s em caso de erro
  - Tracking via `window.FirebaseMonitor`
- **Bootstrap:** `src/index.js` ‚Äî `activeDowntimesLive.start()` ap√≥s `initBridge()`
- **Integra√ß√µes (7 arquivos, 12 locais):**
  - `script.js` ‚Äî `getActiveDowntimesCached()`, `checkActiveDowntimes()`, `getActiveMachineDowntime()`, polling em `setupNewPlanningListeners()`
  - `src/controllers/planning.controller.js` ‚Äî polling + `checkActiveDowntimes()`
  - `src/controllers/launch.controller.js` ‚Äî `loadStandaloneActiveDowntimes()`, `showCardFinishDowntimeModal()`, `confirmCardFinishDowntime()`
  - `src/controllers/downtime-grid.controller.js` ‚Äî `loadStandaloneActiveDowntimes()`, `showCardFinishDowntimeModal()`, `confirmCardFinishDowntime()`
- **Economia estimada:** ~16.750 leituras/dia (~97% redu√ß√£o em `active_downtimes`)
  - Antes: 2 pollings √ó 26 docs √ó 288 polls/dia = ~14.976 reads + ~2.300 reads on-demand
  - Depois: 26 reads iniciais + ~500 reads/dia (apenas docs alterados) + 0 reads on-demand

### 4K. includeMetadataChanges para evitar leituras em reconex√£o (A√ß√£o 3.4)
- **Integrado diretamente no ActiveDowntimesLiveService** (n√£o √© um componente separado)
- **Mecanismo:** `snapshot.metadata.fromCache` filtra re-emiss√µes de cache local em reconex√µes
- **Economia estimada:** ~2.000-5.000 leituras/dia (dependendo da estabilidade da rede)

### 4L. Batch reads com `in` queries para `production_orders` (A√ß√£o 3.5)
- **Arquivos modificados:**
  - `script.js` ‚Äî render dentro de `setupNewPlanningListeners()`: loop de N `.doc(orderId).get()` ‚Üí batch `where(documentId(), 'in', chunk).get()` com chunks de 10
  - `src/controllers/planning.controller.js` ‚Äî mesma refatora√ß√£o: `Promise.all(N individuais)` ‚Üí `Promise.all(ceil(N/10) chunks)`
- **Padr√£o:** Coleta IDs √∫nicos ‚Üí divide em chunks de 10 ‚Üí `Promise.all` de queries `in` ‚Üí mapeia resultados
- **Economia estimada:** Se N=26 m√°quinas, reduz de 26 leituras para 3 queries (ceil(26/10) = 3), economia de ~88% por render
- **Nota:** `BatchQueryManager` em `script.js` L955 j√° existia mas n√£o era utilizado nestas fun√ß√µes

### Resumo de economia N√≠vel 3:
| A√ß√£o | Economia/dia | % Redu√ß√£o |
|------|-------------|-----------|
| 3.2 onSnapshot compartilhado | ~16.750 reads | ~97% em `active_downtimes` |
| 3.4 includeMetadataChanges | ~2.000-5.000 reads | evita duplicatas em reconex√£o |
| 3.5 Batch reads `in` queries | ~5.000-10.000 reads | ~88% nos loops de `production_orders` |
| **Total N√≠vel 3** | **~23.750-31.750 reads/dia** | ‚Äî |

### Novos arquivos criados nesta fase:
| Arquivo | Linhas | Fun√ß√£o |
|---------|--------|--------|
| `src/services/active-downtimes-live.service.js` | ~260 | onSnapshot compartilhado para `active_downtimes` |

---

## Fase 4D ‚Äî Otimiza√ß√µes Arquiteturais N√≠vel 4 (Fev/2026) ‚úÖ

### 4M. enablePersistence com synchronizeTabs (A√ß√£o 4.1)
- **Arquivos modificados:**
  - `script.js` (~L1510) ‚Äî init principal do app
  - `dashboard-tv.html` (~L3057) ‚Äî init da TV de ch√£o de f√°brica
  - `acompanhamento-turno.html` (~L480) ‚Äî init do formul√°rio de turno
- **C√≥digo:** `db.enablePersistence({ synchronizeTabs: true })`
- **N√£o aplicado em:** `admin-fix-downtime.html` (ferramenta diagn√≥stica com config din√¢mica)
- **Comportamento:**
  - Firestore cacheia documentos localmente no IndexedDB
  - Em page reloads, dados s√£o servidos do cache local primeiro (0 reads), depois sincronizados em background
  - `synchronizeTabs: true` permite que m√∫ltiplas abas compartilhem o mesmo cache
  - Se falhar (multiple tabs sem suporte, browser antigo), o app continua normalmente
- **Error handling:** Captura `failed-precondition` e `unimplemented` sem interromper o app
- **Economia estimada:** ~30-50% em leituras de page reload e reconex√£o (~117.000-195.000 reads/dia)

### 4N. TTL Policy para cole√ß√µes de logs (A√ß√£o 4.4)
- **Status:** Campos `timestamp` (serverTimestamp) verificados em todos os writes ‚Äî prontos para TTL
- **Configura√ß√£o no Firebase Console:**
  1. Abrir [Firebase Console](https://console.firebase.google.com/) ‚Üí Projeto Hokkaido
  2. Ir em **Firestore Database** ‚Üí **TTL** (ou **Data retention**)
  3. Adicionar pol√≠tica para `system_logs`: campo `timestamp`, reten√ß√£o **90 dias**
  4. Adicionar pol√≠tica para `hourly_production_entries`: campo `timestamp`, reten√ß√£o **30 dias**

  | Cole√ß√£o | Campo TTL | Reten√ß√£o | Docs antigos removidos |
  |---------|-----------|----------|------------------------|
  | `system_logs` | `timestamp` | 90 dias | ~50.000+ |
  | `hourly_production_entries` | `timestamp` | 30 dias | ~10.000+ |

- **‚ö†Ô∏è ATEN√á√ÉO:** Dados removidos pelo TTL s√£o permanentemente deletados.

### 4O. Avalia√ß√£o: Firestore Lite SDK (A√ß√£o 4.2) ‚Äî ‚ùå BLOQUEADO
- Codebase usa Firebase v8 compat SDK via CDN. Firestore Lite requer v9+ modular SDK.
- Pr√©-requisito: migra√ß√£o completa do SDK (1-2 semanas). N√£o justificado pelo ganho (~10% leituras).

### 4P. Avalia√ß√£o: Firestore Bundles (A√ß√£o 4.3) ‚Äî ‚ùå N√ÉO RECOMENDADO
- Requer Cloud Functions + CDN + deploy pipeline + monitoramento. Custo-benef√≠cio desfavor√°vel.
- O `enablePersistence()` (4.1) j√° resolve o problema de inicializa√ß√£o ‚Äî dados no IndexedDB local.

### Resumo de economia N√≠vel 4:
| A√ß√£o | Economia/dia | Status |
|------|-------------|--------|
| 4.1 enablePersistence | ~117.000-195.000 reads | ‚úÖ Implementado |
| 4.4 TTL Policy | storage + query speed | üìã Config Console pendente |
| 4.2 Firestore Lite | ‚Äî | ‚ùå Bloqueado (requer v9 SDK) |
| 4.3 Firestore Bundles | ‚Äî | ‚ùå N√£o recomendado |

